{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqrCd2eZqxsz",
    "outputId": "d991328d-b1a5-4178-8f06-ffc5450f8e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.8MB 5.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 8.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 133kB 25.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 18.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
      "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq wandb\n",
    "import wandb\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.datasets import fashion_mnist # Used only for loading the data\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwm0W1wW90-X",
    "outputId": "64ded552-7d62-4d69-f675-22cb8defba29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")\n",
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/DL_A4\")\n",
    "from RBM_model import RBM\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-exnstGLR3U-",
    "outputId": "b87e1f33-c8af-4a45-e156-7949f1da06ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "(10000, 784) (10000,) (2500, 784) (2500,) (2500, 784) (2500,)\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Preparing the data\n",
    "###############################\n",
    "# Loading the pre-shuffled fashion mnist dataset\n",
    "(X_fashion_train, y_fashion_train), (X_fashion_test, y_fashion_test) = fashion_mnist.load_data()\n",
    "# Using only a part of the training data and splitting it into training and validation sets\n",
    "X_train = (X_fashion_train.reshape(60000, 784)[:10000, :] > 127).astype(float)\n",
    "y_train = y_fashion_train[:10000]\n",
    "X_val = (X_fashion_train.reshape(60000, 784)[10000 : 12500, :] > 127).astype(float)\n",
    "y_val = y_fashion_train[10000 : 12500]\n",
    "# Using only a part of the test data\n",
    "X_test = (X_fashion_test.reshape(10000, 784)[:2500, :] > 127).astype(float)\n",
    "y_test = y_fashion_test[:2500]\n",
    "# Checking the shapes\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zM5BLtlWR8Fe"
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Listing the hyperparameters in a wandb config \n",
    "###################################################\n",
    "sweep_config = {'name': 'k_10_complete', 'method': 'grid'}\n",
    "sweep_config['metric'] = {'name': 'val_acc', 'goal': 'maximize'}\n",
    "parameters_dict = {\n",
    "                   'num_hidden_vars': {'values': [64, 128, 256]}, # n\n",
    "                   'num_steps_converge': {'values': [10]}, # k\n",
    "                   'CD_etas': {'values': [0.001, 0.005, 0.01, 0.1]}, # eta\n",
    "                  }\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-f-UsgYPR9Pm"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#####################################\n",
    "# Defining the train function\n",
    "#####################################\n",
    "def RBM_wandb_logs(config=sweep_config):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.init().config\n",
    "        wandb.run.name = 'nh_{}_k_{}_CD_{}'.format(config.num_hidden_vars,\\\n",
    "                                                   config.num_steps_converge, \\\n",
    "                                                   config.CD_etas)\n",
    "        \n",
    "        ###########################################\n",
    "        # Training a classifier using RBM\n",
    "        ###########################################\n",
    "        num_visible_vars = 784\n",
    "        epochs = 10\n",
    "\n",
    "        model = RBM(num_visible=num_visible_vars, num_hidden=config.num_hidden_vars)\n",
    "        model.train(input_data=X_train, train_type=\"CD\", epochs=epochs, \\\n",
    "                    k=config.num_steps_converge, eta=config.CD_etas)\n",
    "\n",
    "        test_acc_hist = []\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            # Training the RBM for one epoch            \n",
    "            # Getting hidden representations of the validation data and test data\n",
    "            W = model.param_hist[\"W\"][epoch]\n",
    "            b = model.param_hist[\"b\"][epoch]\n",
    "            c = model.param_hist[\"c\"][epoch]\n",
    "\n",
    "            X_train_hidden = model.sample_h(W, c, X_train.T).T\n",
    "            X_val_hidden = model.sample_h(W, c, X_val.T).T\n",
    "            X_test_hidden = model.sample_h(W, c, X_test.T).T\n",
    "            \n",
    "            classifier = LogisticRegression(max_iter=500)\n",
    "            classifier.fit(X_train_hidden, y_train)\n",
    "            \n",
    "            train_pred = classifier.predict(X_train_hidden)\n",
    "            val_pred = classifier.predict(X_val_hidden)\n",
    "            test_pred = classifier.predict(X_test_hidden)\n",
    "\n",
    "            train_acc = np.sum(train_pred==y_train)/y_train.shape\n",
    "            val_acc = np.sum(val_pred==y_val)/y_val.shape\n",
    "            test_acc = np.sum(test_pred==y_test)/y_test.shape\n",
    "\n",
    "            train_pred = classifier.predict_proba(X_train_hidden)\n",
    "            val_pred = classifier.predict_proba(X_val_hidden)\n",
    "            test_pred = classifier.predict_proba(X_test_hidden)\n",
    "\n",
    "            train_loss = log_loss(y_train, train_pred)\n",
    "            val_loss = log_loss(y_val, val_pred)\n",
    "            test_loss = log_loss(y_test, test_pred)\n",
    "\n",
    "            wandb.log({\"train_acc\": train_acc, \"val_acc\": val_acc, \\\n",
    "                       \"test_acc\": test_acc, \"train_loss\": train_loss, \\\n",
    "                       \"val_loss\": val_loss, \"test_loss\": test_loss, \\\n",
    "                       \"RBM_train_loss\": model.overall_loss[epoch]})\n",
    "\n",
    "            pickling_on = open(wandb.run.name+\".pickle\",\"wb\")\n",
    "            pickle.dump(model, pickling_on)\n",
    "            pickling_on.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "54p9I70DAdPV"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Run without wandb\n",
    "#################################\n",
    "num_hidden_vars = 256\n",
    "num_steps_converge = 30\n",
    "CD_etas = 0.1\n",
    "\n",
    "model = RBM(num_visible=784, num_hidden=num_hidden_vars)\n",
    "model.train(input_data=X_train, train_type=\"CD\", epochs=1, \\\n",
    "            k=num_steps_converge, \\\n",
    "            eta=CD_etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660,
     "referenced_widgets": [
      "b828d3aac28d491195bea8e92c6572f1"
     ]
    },
    "id": "7qZDYuFESGx7",
    "outputId": "85664133-204a-4983-992a-454498311746"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Setting up wandb sweeps\n",
    "#################################\n",
    "sweep_id = wandb.sweep(sweep_config, project = 'DL-Assignment4-Q4')\n",
    "wandb.agent(sweep_id, function = RBM_wandb_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgf_zD-9-rUo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_A4_Q5_k_10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
